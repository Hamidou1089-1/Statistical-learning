Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError[degree] = mean(cvError_k)
}
# get the final error on the test set
x = X_test
y = Y_test
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# split the dataset into CV and test partitions
N_test = 50
X_test <- X[1:N_test]
Y_test <- y[1:N_test]
N_cv = N - N_test
X_cv <- X[(N_test+1):N]
Y_cv <- Y[(N_test+1):N]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 20
# create array to store cvError on each degree
cvError_k <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError[degree] = mean(cvError_k)
}
# get the final error on the test set
x = X_test
y = Y_test
cvError
cvError_k
X_val
f
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# split the dataset into CV and test partitions
N_test = 50
X_test <- X[1:N_test]
Y_test <- y[1:N_test]
N_cv = N - N_test
X_cv <- X[(N_test+1):N]
Y_cv <- Y[(N_test+1):N]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 10
# create array to store cvError on each degree
cvError_k <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError[degree] = mean(cvError_k)
}
# get the final error on the test set
x = X_test
y = Y_test
length(X_val)
length(X_train)
cvError
length(cvError)
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# split the dataset into CV and test partitions
N_test = 50
X_test <- X[1:N_test]
Y_test <- y[1:N_test]
N_cv = N - N_test
X_cv <- X[(N_test+1):N]
Y_cv <- Y[(N_test+1):N]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 10
# create array to store cvError on each degree
cvError_k <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError[degree] = mean(cvError_k)
}
# get the final error on the test set
x = X_test
y = Y_test
cvError
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# split the dataset into CV and test partitions
N_test = 50
X_test <- X[1:N_test]
Y_test <- y[1:N_test]
N_cv = N - N_test
X_cv <- X[(N_test+1):N]
Y_cv <- Y[(N_test+1):N]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 10
# create array to store cvError on each degree
cvError_degree <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError_degree[degree] = mean(cvError_k)
}
# get the final error on the test set
x = X_test
y = Y_test
cvError_degree
min(cvError_degree)
which.min(cvError_degree)
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# split the dataset into CV and test partitions
N_test = 50
X_test <- X[1:N_test]
Y_test <- y[1:N_test]
N_cv = N - N_test
X_cv <- X[(N_test+1):N]
Y_cv <- Y[(N_test+1):N]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 10
# create array to store cvError on each degree
cvError_degree <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError_degree[degree] = mean(cvError_k)
}
# get the degree with the minimum CV error
degree_min = which.min(cvError_degree)
# estimate a model on this degree with whole CV dataset
x = X_cv
y = Y_cv
f <- lm(y ~ poly(x, degree=degree_min, raw=TRUE))
# estimate the error of the model on the test dataset
error <- predict(object=f, newdata=data.frame(x=X_test)) - Y_test
testError = mean(error^2)
testError
cvError_k
cvError_degree
degree_min
cvError
cvError_degree
cvError_k
testError
sample(1:10, replace = )
sample(1:10, replace=FALSE)
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# randomly split the dataset into CV and test partitions
idx_random = sample(1:N, replace=FALSE)
N_test = 50
X_test <- X[idx_random[1:N_test]]
Y_test <- y[idx_random[1:N_test]]
N_cv = N - N_test
X_cv <- X[idx_random((N_test+1):N)]
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# randomly split the dataset into CV and test partitions
idx_random = sample(1:N, replace=FALSE)
N_test = 50
X_test <- X[idx_random[1:N_test]]
Y_test <- y[idx_random[1:N_test]]
N_cv = N - N_test
X_cv <- X[idx_random[(N_test+1):N]]
Y_cv <- Y[idx_random[(N_test+1):N]]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 10
# create array to store cvError on each degree
cvError_degree <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError_degree[degree] = mean(cvError_k)
}
# get the degree with the minimum CV error
degree_min = which.min(cvError_degree)
# estimate a model on this degree with whole CV dataset
x = X_cv
y = Y_cv
f <- lm(y ~ poly(x, degree=degree_min, raw=TRUE))
# estimate the error of the model on the test dataset
error <- predict(object=f, newdata=data.frame(x=X_test)) - Y_test
testError = mean(error^2)
testError
X_cv
Y_cv
error
X_cv
Y_cv
f
predict(X_cv)
predict(f)
predict(f) - Y_cv
mean(predict(f) - Y_cv)
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# randomly split the dataset into CV and test partitions
idx_random = sample(1:N, replace=FALSE)
N_test = 50
X_test <- X[idx_random[1:N_test]]
Y_test <- y[idx_random[1:N_test]]
N_cv = N - N_test
X_cv <- X[idx_random[(N_test+1):N]]
Y_cv <- Y[idx_random[(N_test+1):N]]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 10
# create array to store cvError on each degree
cvError_degree <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError_degree[degree] = mean(cvError_k)
}
# get the degree with the minimum CV error
degree_min = which.min(cvError_degree)
# estimate a model on this degree with whole CV dataset
x = X_cv
y = Y_cv
f <- lm(y ~ poly(x, degree=degree_min, raw=TRUE))
# estimate the error of the model on the test dataset
error <- predict(object=f, newdata=data.frame(x=X_test)) - Y_test
testError = mean(error^2)
error
X_tes
X_test
Y_test
# set the seed for reproducible results
set.seed(1)
# how many samples to generate
N = 200
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# randomly split the dataset into CV and test partitions
idx_random = sample(1:N, replace=FALSE)
N_test = 50
X_test <- X[idx_random[1:N_test]]
Y_test <- Y[idx_random[1:N_test]]
N_cv = N - N_test
X_cv <- X[idx_random[(N_test+1):N]]
Y_cv <- Y[idx_random[(N_test+1):N]]
# how many folds
K = 5
# choose the maximum degree of the polynomial to approximate data
degree_max = 10
# create array to store cvError on each degree
cvError_degree <- vector(mode="numeric",length=K)
# loop through the degrees
for (degree in 1:degree_max)
{
# create array to store the errors for each fold
cvError_k <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N_cv-K, by=K)
# get the training dataset
X_train <- X_cv[-idx_val];
Y_train <- Y_cv[-idx_val]
# get the testing dataset
X_val <- X_cv[idx_val]
Y_val <- Y_cv[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
cvError_k[k] <- mean(error^2)
}
# get the average cvError across the folds
cvError_degree[degree] = mean(cvError_k)
}
# get the degree with the minimum CV error
degree_min = which.min(cvError_degree)
# estimate a model on this degree with whole CV dataset
x = X_cv
y = Y_cv
f <- lm(y ~ poly(x, degree=degree_min, raw=TRUE))
# estimate the error of the model on the test dataset
error <- predict(object=f, newdata=data.frame(x=X_test)) - Y_test
testError = mean(error^2)
testError
cvError
cvError_k
testError
NAm2 = read.table("NAm2.txt", header=TRUE)
setwd("~/Courses/ENSIMAG/SADM-local/2023/TP/TP2")
NAm2 = read.table("NAm2.txt", header=TRUE)
NAm2
colnames(NAm2)
