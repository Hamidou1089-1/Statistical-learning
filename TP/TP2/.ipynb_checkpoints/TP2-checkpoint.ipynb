{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecfe602-4165-410f-b1ab-dc8a517e32db",
   "metadata": {},
   "source": [
    "### ENSIMAG ‚Äì Grenoble INP ‚Äì UGA - Academic year 2024-2025\n",
    "# Introduction to Statistical Learning and Applications ([website](https://github.com/ISLA-Grenoble/2025-main))\n",
    "\n",
    "- Pedro L. C. Rodrigues -- `pedro.rodrigues@inria.fr`\n",
    "\n",
    "- Alexandre Wendling -- `alexandre.wendling@univ-grenoble-alpes.fr`\n",
    "\n",
    "***\n",
    "\n",
    "### ‚ö†Ô∏è General guidelines for TPs\n",
    "\n",
    "Each team shall upload its report on [Teide](https://teide.ensimag.fr/) before the deadline indicated at the course website. Please\n",
    "**include the name of all members** of the team on top of your report.\n",
    "The report should contain graphical representations and explanatory text. For each graph, axis names should be provided as well\n",
    "as a legend when it is appropriate. Figures should be explained by a few sentences in the text. Answer to\n",
    "the questions in order and refer to the question number in your report. Computations and\n",
    "graphics have to be performed in `python`. The report should be written as a jupyter notebook. This is a file format that allows users to format documents containing text written in markdown and `python` instructions. You should include all of the `python` instructions that you have used in the document so that it may be possible to replicate your results.\n",
    "\n",
    "***\n",
    "\n",
    "# üñ•Ô∏è TP2: Principal components regression in genetics\n",
    "\n",
    "The goal of this TP session is to use genetic markers to predict the geographical origin of a set of indians from South, Central, and North America. We propose to build two regression linear models to predict the latitude and longitude of an individual based on its genetic markers. Because the number of markers (p = 5709) is larger than the number of samples (N = 494), the predictors of the regression model will be the outputs of a principal component analysis (PCA) performed on the genetic markers. A genetic marker is encoded 1 if the individual has a mutation, 0 elsewhere.\n",
    "\n",
    "## ‚ñ∂Ô∏è Exercise 1: Data visualization (1 point)\n",
    "\n",
    "NB: To do this exercise you will have to install packages `geopandas` and `geodatasets`.\n",
    "\n",
    "Download dataset `NAm2.txt` from [here](https://github.com/ISLA-Grenoble/2025-main/blob/main/TP/TP2/NAm2.txt). Each row of the dataset corresponds to an individual and the columns have explicit names. The third column contains the names of the tribes to which each individual pertains. Columns 7 and 8 contain the latitude and the longitude and from Column 9 onwards are genetic markers, which are encoded are 0 or 1. Run the code described below and explain how it works.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geodatasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "file_path = 'NAm2.txt'\n",
    "df = pd.read_csv(file_path, delimiter=' ')\n",
    "\n",
    "# Extract relevant columns\n",
    "latitude = df.iloc[:, 6]\n",
    "longitude = df.iloc[:, 7]\n",
    "tribes = df.iloc[:, 2]\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(longitude, latitude))\n",
    "\n",
    "# Plotting\n",
    "world = gpd.read_file(geodatasets.get_path('naturalearth.land'))\n",
    "fig, ax = plt.subplots(figsize=(8.0, 6.5))\n",
    "plt.subplots_adjust(left=0.0, right=0.90, bottom=0.10, top=0.92)\n",
    "world.clip([-140, -55, -25, 75]).plot(ax=ax, color='white', edgecolor='black')\n",
    "marker_list = ['o', 'v', 's']\n",
    "colors_list = [f'C{i}' for i in range(9)]\n",
    "for i, tribe in enumerate(gdf['Pop'].unique()):\n",
    "    members_tribe = gdf[gdf['Pop'] == tribe]\n",
    "    ax.scatter(members_tribe['long'], members_tribe['lat'], \n",
    "               marker=marker_list[i//9], \n",
    "               color=colors_list[i%9], label=tribe)\n",
    "ax.legend(loc='center right', bbox_to_anchor=(1.4, 0.5))\n",
    "ax.set_title('Tribes Locations')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "fig.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fddcc4-00bc-4b3e-87b3-ed236c850ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0578f36",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Exercise 2: Multiple linear regression (2 points)\n",
    "\n",
    "Using **only** the genetic markers as predictors, you will estimate a multiple linear regression model to predict the longitude of each individual.\n",
    "\n",
    "You will proceed in several steps.\n",
    "\n",
    "**(a)** First, try to estimate the coefficients of the multiple linear regression using the expression seen in class \n",
    "\n",
    "$$\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y$$\n",
    "\n",
    "You should proceed as we did in TP1 using `numpy.linalg.solve` to obtain the values of $\\beta$. \n",
    "\n",
    "Did you run into any errors? What is going on? Relate your answer to the fact that $\\text{rank}(X) < p$, where $X \\in R^{N*p}$ is the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce4864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c155802d",
   "metadata": {},
   "source": [
    "**(b)** Use function `numpy.linalg.lstsq` to estimate the coefficients (it may take a few seconds to get a result). \n",
    "\n",
    "And now? Did you get any errors? Why is that? \n",
    "\n",
    "Relate your answer to the difference between functions `numpy.linalg.solve` and `numpy.linalg.lstsq`.\n",
    "\n",
    "You can check the documention for both functions as well as [this](https://netlib.org/lapack/lug/node27.html) link for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef851098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca2ecfae-cfda-4364-adc0-01a72d28cc0a",
   "metadata": {},
   "source": [
    " **(c)** We will now use `sklearn` to do our linear regression with the help of class `sklearn.linear_model.LinearRegression` whose documentation is available [here](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html). Note that every estimator from `sklearn` has a `fit` and a `predict` method, which are used to calculate coefficients and predict values (see [here](https://scikit-learn.org/stable/getting_started.html#fitting-and-predicting-estimator-basics) for more info). In our current case, we can do:\n",
    "\n",
    "```\n",
    "# select only the genetic markers as predictors\n",
    "predictors = df.columns[8:]\n",
    "# create the design matrix\n",
    "X = df[predictors].values\n",
    "# get the observed values to predict\n",
    "y = df['long']\n",
    "# fit a multiple linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "```\n",
    "\n",
    "You should not run into errors now, since `sklearn` also uses `lstsq` to solve the normal equations, as shown [here](https://github.com/scikit-learn/scikit-learn/blob/d666202a9349893c1bd106cc9ee0ff0a807c7cf3/sklearn/linear_model/_base.py#L682) (though it uses the `scipy` implementation instead of the `numpy` for \"historical\" reasons). Check the values of the estimated coefficients stored as an attribute in `lr.coef_`, are they the same as the ones obtained in item **(b)**? Probably not. This is because `sklearn` re-centers the predictors before estimating the coefficients of the linear regression, as shown [here](https://github.com/scikit-learn/scikit-learn/blob/d666202a9349893c1bd106cc9ee0ff0a807c7cf3/sklearn/linear_model/_base.py#L622). What would be a practical reason for doing such re-centering systematically? Hint: it has to do with how to interpret the intercept of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c8a45-2de6-4e83-9dd9-060b28e82793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3da997",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Exercise 3: Principal components analysis (5 points)\n",
    "\n",
    "**(a)** Explain in a few words the main concepts and ideas underlying the principal component analysis (PCA). You should include both the geometric and statistical interpretations of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34fe3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e91efb",
   "metadata": {},
   "source": [
    "**(b)** Use the estimator defined in `sklearn.decomposition.PCA` to do a PCA on the dataset. Plot the first two dimensions of the projected data points on a scatterplot. The scattered points should have different markers and colors depending on which tribe they belong to. You can use the same color/marker style from **Exercise 2** or propose a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0789ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f22c062",
   "metadata": {},
   "source": [
    "**(c)** Remember from our class that the results of PCA are affected when pre-processing transformations are applied to the data. We will illustrate this using `sklearn.preprocessing.StandardScaler` as per:\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_std = scaler.transform(X)\n",
    "```\n",
    "Redo the 2D scatter plot from item **(b)** on the normalized version of the datast. How does it compare to your previous plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0e114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede2abf7-e3e8-4f29-bbb2-1aef315c4b65",
   "metadata": {},
   "source": [
    "**(d)** Given the results in **(b)** and **(c)**, what can you conclude regarding the necessity of standardizing the data points for the dataset consider in this TP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5fa95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372e3878",
   "metadata": {},
   "source": [
    "**(e)** Which percentage of variance is captured by the first two principal components? How many principal components would you keep if you would like to represent the genetic markers using a minimal number of principal components? To help answering this question, you can use a plot showing the cumulative percentage of variance as a function of the number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f131b2-edc6-4567-a92a-dd3657ca6cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf8734d4-2df6-4f7d-a088-b67a9dae23b0",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Exercise 4: Principal components regression (4 points)\n",
    "\n",
    "**(a)** Predict the latitude and the longitude of all points from the dataset using the scores of the first 250 PCA axes. Plot the predicted spatial coordinates using the same style and structure from **Exercise 1** and compare the results from each plot. What can you conclude? Does the new map illustrate somehow too optimistically (or too pessimistically) the ability to find geographical origin of individuals outside the database from its genetic markers? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f80390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "734fec03",
   "metadata": {},
   "source": [
    "**(b)** Quantify the error of the linear regression model using the mean distance between real and predicted coordinates. Beware to use `sklearn.metrics.pairwise.haversine_distances` so to correctly measure the distances between points so to take into account the curvature of the Earth. Your answer should be given in kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df214b-359b-482c-a9b0-58eaac2c8659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a82eb6b5",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Exercise 5: PCR and cross-validation (6 points)\n",
    "\n",
    "Our goal now is to build the best model to predict individual geographical coordinates. \n",
    "\n",
    "For this, you will run a linear regression to predict latitudes and longitudes. Note that `sklearn.linear_model.LinearRegression` can naturally handle the fact of having two sets of coefficients. We will use ten-fold cross-validation to helps us choose the number of principal axes that we should keep. You should report the errors in terms of kilometers as done in **Exercise 4(b)**.\n",
    "\n",
    "**(a)** Recall in a few words the principle of cross-validation. Explain why this procedure is useful when building a predictive model. Your answer should mention different strategies to handle datasets in which the samples are not IID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df67a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b90edf0",
   "metadata": {},
   "source": [
    "**(b)** Based on the structure of the dataset being used, such as the different countries of the individuals and the order in which the rows of the dataframe are provided, explain which choice of cross-validation iterator from [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators) seems the most adequate for our context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1d216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84ccd721",
   "metadata": {},
   "source": [
    "**(c)** We first assess the quality of the PCR fit for `n_components=4`. Note that you should be careful in avoiding [data leakage](https://scikit-learn.org/stable/common_pitfalls.html#data-leakage) problems when doing the PCA followed by a multiple linear regression. You should use the pipeline interface from scikit-learn with `sklearn.pipeline.make_pipeline` to facilitate your task. Be sure to evaluate the errors as done in **Exercise 4(b)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097072f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdeb2c9b",
   "metadata": {},
   "source": [
    "**(d)** Repeat the analysis from item **(b)** but changing `n_components` between 2 and 440 in steps of 10. Plot the mean training and test errors versus the number of principal components. Attention, the errors should be given in kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b203a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "172a5e16-936d-4d90-b4e6-c7aca1e18081",
   "metadata": {},
   "source": [
    "**(e)** Which model would you keep? What is the prediction error for this model? Compare it with its corresponding training error. Plot the predicted coordinates on a map as in **Exercise 4(a)**. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2f203-a0d6-4b78-8fd9-fb48819c4c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b24737-191a-4295-8641-05581390b568",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Exercise 6: Conclusion (2 points)\n",
    "\n",
    "Propose a conclusion to your study. You can write a paragraph about the quality of predictors versus the number of factors, possible improvements to the approach (for instance, showing what happens when using [partial least squares](https://scikit-learn.org/1.5/auto_examples/cross_decomposition/plot_pcr_vs_pls.html) instead of PCR), comment on the performance of the regression in predictions for each country separately, etc. Note that we expect a thorough presentation of the final predictive model as well as an interpretation of it, not simply a bunch of `python` code lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7494e5-677d-4076-b430-cd1cd5138a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isla2025",
   "language": "python",
   "name": "isla2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
