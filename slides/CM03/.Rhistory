testError_array[k] <- mean(error^2)
}
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree = 2
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, 80, by=20)
# get the training dataset
X_train <- X[-idx_val];
y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
f <- lm(y_train ~ poly(x_train, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(w=x_val)) - y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree = 2
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, 80, by=20)
# get the training dataset
X_train <- X[-idx_val];
y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
f <- lm(y_train ~ poly(X_train, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(w=X_val)) - y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree = 2
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, 80, by=20)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError_array
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree = 2
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, 80, by=20)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
print(mean(testError_array))
idx_val
seq(0, 80)
seq(0, 100)
seq(0, 100)
seq(0, 95, by=5)
dim(seq(0, 95, by=5))
len(seq(0, 95, by=5))
numel(seq(0, 95, by=5))
length(seq(0, 95, by=5))
seq(0, N-K, by=K)
N = 20
K = 10
seq(0, N-K, by=K)
N = 100
seq(0, N-K, by=K)
K
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
for (degree in 1:20)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
print(mean(testError_array))
}
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError_array <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
plot(degree_max)
testError
plot(testError)
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
plot(testError)
lines(testError)
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
plot(testError, xlab='degree', ylab='cvError')
lines(testError)
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
pdf('example-cross-validation.pdf', width=12, height=5)
plot(testError, xlab='degree', ylab='cvError')
lines(testError)
dev.off()
setwd("~/Courses/ENSIMAG/SADM-local/2023/CM/CM3")
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
pdf('example-cross-validation.pdf', width=12, height=5)
plot(testError, xlab='degree', ylab='cvError')
lines(testError)
dev.off()
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
pdf('example-cross-validation.pdf', width=8, height=6)
plot(testError, xlab='degree', ylab='cvError')
lines(testError)
dev.off()
# set the seed for reproducible results
set.seed(1)
# how many samples to consider
N = 100
# choose the values of xi
X <- seq(-5, 5, length=N)
# generate the observations yi
Y <- X - X^2 + X^3 + rnorm(N, mean=0, sd=20)
# choose the degree of the polynomial to approximate data
degree_max = 20
testError <- vector(mode="numeric",length=degree_max)
for (degree in 1:degree_max)
{
# how many folds
K = 5
# create array to store the errors for each fold
testError_array <- vector(mode="numeric",length=K)
for (k in 1:K)
{
# choose the indices of the samples for the fold k-th fold
idx_val <- k + seq(0, N-K, by=K)
# get the training dataset
X_train <- X[-idx_val];
Y_train <- Y[-idx_val]
# get the testing dataset
X_val <- X[idx_val]
Y_val <- Y[idx_val]
# estimate the parameters of a polynomial fit to the data
x = X_train
y = Y_train
f <- lm(y ~ poly(x, degree=degree, raw=TRUE))
# get the prediction error to estimate data from validation set
error <- predict(object=f, newdata=data.frame(x=X_val)) - Y_val
# calculate the mean squared error for this fold
testError_array[k] <- mean(error^2)
}
testError[degree] = mean(testError_array)
}
pdf('example-cross-validation.pdf', width=7, height=6)
plot(testError, xlab='degree', ylab='cvError')
lines(testError)
dev.off()
